# ğŸš€ RAG: Local Server Deployable Retrieval-Augmented Generation (RAG)

## ğŸ“Œ Overview
This project provides a **Retrieval-Augmented Generation (RAG)** pipeline designed for **local deployment** with **data security**. It can infer over any document(s) provided by the user. The system is **general-purpose**, allowing users to upload PDFs and run **semantic search & question-answering** without external dependencies.

---

## ğŸ”¥ Features
- **Local Server Deployment** (No Cloud Dependency)
- **Secure Data Processing**
- **Plug & Play with Any Document**
- **Supports Fine-Tuning and Custom Datasets**
- **Evaluation Metrics:** BLEU, ROUGE (1,2,L), Cosine Similarity

---

## ğŸ—ï¸ Project Structure
```plaintext
â”œâ”€â”€ chroma/                         # ChromaDB storage (delete if errors occur)
â”œâ”€â”€ data/                            # User-uploaded PDFs (add files here)
â”œâ”€â”€ evaluation_outputs/              # Precomputed evaluation results
â”‚   â”œâ”€â”€ Cyber_Security/
â”‚   â”œâ”€â”€ Laws/
â”‚   â”œâ”€â”€ Monopoly/
â”œâ”€â”€ output/                          # Generated outputs (QA pairs, embeddings)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ populate_database.py         # Prepares database from PDFs
â”‚   â”œâ”€â”€ query_data.py                # Query your documents
â”‚   â”œâ”€â”€ metrics_data_gen.py          # Evaluation metrics computation
â”‚   â”œâ”€â”€ Question_Answer_Generator.py # Generates QA pairs
â”‚   â”œâ”€â”€ get_embedding_function.py    # Embedding function setup
â”œâ”€â”€ README.md                        # This guide
â”œâ”€â”€ requirements.txt                  # Dependencies
```

---

## ğŸ› ï¸ Installation

### 1ï¸âƒ£ Prerequisites
Ensure your system has:
- **Python 3.10+**
- **[Ollama](https://ollama.com)**
- **ChromaDB**
- **Llama 3.2**
- **nomic-embed-text**

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Usage Guide

### ğŸ—ï¸ Step 1: Add Documents
Place your PDFs inside the `data/` directory.

### ğŸ”„ Step 2: Populate the Database
Run the script to process and embed your documents:
```bash
python scripts/populate_database.py
```
> If you face issues, **delete the `chroma/` folder** and rerun the script.

### ğŸ¤– Step 3: Query the RAG System
Once the database is populated, you can retrieve information using:
```bash
python scripts/query_data.py "Your query here"
```

### ğŸ“Š Step 4: Evaluate Performance (Optional)
If you want to compute metrics like BLEU, ROUGE, and Cosine Similarity:
```bash
python scripts/metrics_data_gen.py
```

---

## ğŸ¯ How It Works

### **ğŸ” Document Ingestion & Chunking**
- Extracts text from PDFs.
- Splits text into **semantic chunks** using `RecursiveCharacterTextSplitter`.

### **ğŸ’¾ Embedding & Storage**
- Uses `nomic-embed-text` for **embedding generation**.
- Stores embeddings in **ChromaDB** for **fast retrieval**.

### **ğŸ” Retrieval & Generation**
- Queries are matched against **ChromaDB** embeddings.
- The best-matched contexts are used to generate responses via **Llama 3.2**.

### **ğŸ“Š Evaluation**
- Computes **BLEU, ROUGE (1,2,L), Cosine Similarity** between retrieved/generated responses and expected answers.

---

## ğŸ› ï¸ Troubleshooting

| Issue | Solution |
|--------|-----------|
| `ModuleNotFoundError: No module named 'chromadb'` | Run `pip install chromadb` |
| `Database not updating after adding new PDFs` | **Delete the `chroma/` folder** and rerun `populate_database.py` |
| `Query results are incorrect or empty` | Ensure the database was populated correctly before running `query_data.py` |
| `Ollama model not found` | Run `ollama pull llama3.2` to download the required model |

---

## ğŸ”® Future Enhancements
- âœ… **API Support**
- âœ… **Web UI Integration**
- âœ… **More Evaluation Metrics**
- âœ… **Fine-Tuning Support**
